{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Achieve higher derivatives\n",
    "\n",
    "这个阶段的主要目标是扩展 DeZero，使其能够计算高阶导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "from dezero import Function\n",
    "import os\n",
    "import subprocess\n",
    "import math\n",
    "from dezero.utils import plot_dot_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 25: Visualization of computational graphs Ⅰ\n",
    "\n",
    "使用Graphviz库，我们可以将计算图可视化为图形。这样可以更直观地理解计算图的结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 26: Visualization of computational graphs Ⅱ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将Variable实例赋给函数，返回DOT语言编写的表示实例信息的字符串\n",
    "def _dot_var(v, verbose=False):\n",
    "    dot_var = '{} [label=\"{}\", color=orange, style=filled]\\n'\n",
    "    name = '' if v.name is None else v.name\n",
    "    # verbose为True时，添加变量的形状和数据类型\n",
    "    if verbose and v.data is not None:\n",
    "        if v.name is not None:\n",
    "            name += ': '\n",
    "        name += str(v.shape) + ' ' + str(v.dtype)\n",
    "    return dot_var.format(id(v), name)  # 使用python内置函数id()获取对象的内存地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1402407802760 [label=\"x\", color=orange, style=filled]\n",
      "\n",
      "1402407802760 [label=\"x: (2, 3) float64\", color=orange, style=filled]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.random.randn(2, 3))\n",
    "x.name = 'x'\n",
    "print(_dot_var(x))\n",
    "print(_dot_var(x, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将Function实例赋给函数，返回DOT语言编写的表示实例信息的字符串\n",
    "def _dot_func(f):\n",
    "    dot_func = '{} [label=\"{}\", color=lightblue, style=filled, shape=box]\\n'\n",
    "    txt = dot_func.format(id(f), f.__class__.__name__)\n",
    "    # 用DOT记述函数与输入变量之间的连接，以及函数与输出变量之间的连接\n",
    "    dot_edge = '{} -> {}\\n' \n",
    "    for x in f.inputs:\n",
    "        txt += dot_edge.format(id(x), id(f))\n",
    "    for y in f.outputs:\n",
    "        txt += dot_edge.format(id(f), id(y()))  # y是weakref\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1404453256520 [label=\"Add\", color=lightblue, style=filled, shape=box]\n",
      "1402407784328 -> 1404453256520\n",
      "1402407783624 -> 1404453256520\n",
      "1404453256520 -> 1404287120264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x0 = Variable(np.array(1.0))\n",
    "x1 = Variable(np.array(1.0))\n",
    "y = x0 + x1\n",
    "txt = _dot_func(y.creator)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考backward函数的实现，实现获取计算图的函数\n",
    "def get_dot_graph(output, verbose=True):\n",
    "    txt = ''\n",
    "    funcs = []\n",
    "    seen_set = set()\n",
    "\n",
    "    def add_func(f):\n",
    "        if f not in seen_set:  # 不需要按辈分顺序添加函数\n",
    "            funcs.append(f)\n",
    "            seen_set.add(f)\n",
    "\n",
    "    add_func(output.creator)\n",
    "    txt += _dot_var(output, verbose)  # 输出变量\n",
    "\n",
    "    while funcs:\n",
    "        func = funcs.pop()\n",
    "        txt += _dot_func(func)  # 函数\n",
    "        for x in func.inputs:\n",
    "            txt += _dot_var(x, verbose)  # 输入变量\n",
    "\n",
    "            if x.creator is not None:\n",
    "                add_func(x.creator)  # 添加函数\n",
    "\n",
    "    return 'digraph g {\\n' + txt + '}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dot_graph(output, verbose=True, to_file='graph.png'):\n",
    "    dot_graph = get_dot_graph(output, verbose)\n",
    "\n",
    "    # 保存为dot文件\n",
    "    tmp_dir = os.path.join(os.path.expanduser('~'), '.dezero')\n",
    "    if not os.path.exists(tmp_dir):  # 创建目录\n",
    "        os.mkdir(tmp_dir)\n",
    "    dot_file = os.path.join(tmp_dir, 'tmp.dot')\n",
    "    with open(dot_file, 'w') as f:\n",
    "        f.write(dot_graph)\n",
    "\n",
    "    # 调用Graphviz的dot命令\n",
    "    ext = os.path.splitext(to_file)[1][1:]  # 获取文件扩展名\n",
    "    cmd = 'dot {} -T {} -o {}'.format(dot_file, ext, to_file)\n",
    "    subprocess.run(cmd, shell=True)\n",
    "\n",
    "    # 在juptyer notebook中显示图像\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        img = Image.open(to_file)\n",
    "        img.show()\n",
    "    except ImportError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = Variable(np.array(1.0))\n",
    "x1 = Variable(np.array(1.0))\n",
    "y = x0 + x1\n",
    "x0.name = 'x0'\n",
    "x1.name = 'x1'\n",
    "y.name = 'y'\n",
    "plot_dot_graph(y, verbose=False, to_file='add.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goldstein(x, y):\n",
    "    z = (1 + (x + y + 1)**2 * (19 - 14*x + 3*x**2 - 14*y + 6*x*y + 3*y**2)) * \\\n",
    "        (30 + (2*x - 3*y)**2 * (18 - 32*x + 12*x**2 + 48*y - 36*x*y + 27*y**2))\n",
    "    return z\n",
    "\n",
    "\n",
    "x = Variable(np.array(1.0))\n",
    "y = Variable(np.array(1.0))\n",
    "z = goldstein(x, y)\n",
    "z.backward()\n",
    "\n",
    "x.name = 'x'\n",
    "y.name = 'y'\n",
    "z.name = 'z'\n",
    "plot_dot_graph(z, verbose=False, to_file='goldstein.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 27: Derivative of Taylor's expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sin(Function):\n",
    "    # 直接使用np.sin()计算正弦函数的值，np.cos()计算导数\n",
    "    def forward(self, x):\n",
    "        y = np.sin(x)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x = self.inputs[0].data\n",
    "        gx = gy * np.cos(x)\n",
    "        return gx\n",
    "    \n",
    "def sin(x):\n",
    "    return Sin()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067811865476\n",
      "0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array(np.pi/4))\n",
    "y = sin(x)\n",
    "y.backward()\n",
    "\n",
    "print(y.data)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用泰勒展开逼近函数：\n",
    "\n",
    "$$\n",
    "f(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\frac{f'''(a)}{3!}(x-a)^3 + \\cdots\n",
    "$$\n",
    "\n",
    "a = 0时，泰勒展开式称为麦克劳林级数。\n",
    "$$\n",
    "f(x) = f(0) + f'(0)x + \\frac{f''(0)}{2!}x^2 + \\frac{f'''(0)}{3!}x^3 + \\cdots\n",
    "$$\n",
    "\n",
    "对于sinx，sin(0) = 0，sin'(0) = cos(0) = 1，sin''(0) = -sin(0) = 0，sin'''(0) = -cos(0) = -1，以此类推可得：\n",
    "$$\n",
    "\\sin x = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sin(x, threshold=1e-4):\n",
    "    y = 0\n",
    "    for i in range(100000):\n",
    "        c = (-1) ** i / math.factorial(2*i+1)\n",
    "        t = c * x ** (2*i+1)\n",
    "        y = y + t\n",
    "        if abs(t.data) < threshold:\n",
    "            break\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071064695751781\n",
      "0.7071032148228457\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array(np.pi/4))\n",
    "y = my_sin(x)\n",
    "y.backward()\n",
    "\n",
    "print(y.data)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dot_graph(y, verbose=False, to_file='my_sin.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 28: Function optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeZero 现在能够自动计算导数了。导数有多种用途，其中最重要的一种用途就是函数优化。\n",
    " \n",
    "优化指的是对于给定的函数，找到使其取得最大值或最小值的函数参数或输入值。\n",
    "\n",
    "本步骤处理Rosenbrock函数，又叫香蕉函数，目标是找到使得Rosenbrock函数取得最小值的x0和x1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x0, x1):\n",
    "    y = 100 * (x1 - x0 ** 2) ** 2 + (1 - x0) ** 2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用梯度下降法求解Rosenbrock函数的最小值\n",
    "x0 = Variable(np.array(0.0))\n",
    "x1 = Variable(np.array(2.0))\n",
    "lr = 0.001\n",
    "iters = 1000\n",
    "\n",
    "for i in range(iters):\n",
    "    print(x0, x1)\n",
    "    y = rosenbrock(x0, x1)\n",
    "    x0.cleargrad()\n",
    "    x1.cleargrad()\n",
    "    y.backward()\n",
    "\n",
    "    x0.data -= lr * x0.grad\n",
    "    x1.data -= lr * x1.grad\n",
    "# 很多次迭代后，x0和x1的值接近于1，1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 29: Optimization using Newton method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一个步骤使用了很多次迭代梯度下降法才找到最终的解，收敛速度较慢。（梯度下降本身也不擅长处理rosenbrock函数，因为rosenbrock函数的梯度在接近最小值时会变得很小，导致梯度下降法收敛速度变慢。）\n",
    "\n",
    "本步骤使用牛顿法，能以更少的步数获取最优解。\n",
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"./res/difference.png\" width=\"500\"/></td>\n",
    "    <td><img src=\"./res/牛顿法.png\" width=\"300\"/></td>\n",
    "  </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "二次近似：（截断泰勒展开式到二次项）\n",
    "$$\n",
    "f(x) \\approx f(x_0) + f'(x_0)(x-x_0) + \\frac{1}{2}f''(x_0)(x-x_0)^2\n",
    "$$\n",
    "二次近似函数的最小值位于：$x = x_0 - \\frac{f'(x_0)}{f''(x_0)}$\n",
    "\n",
    "梯度下降法，牛顿法的迭代公式分别为：\n",
    "$$\n",
    "x_{new} = x_{old} - \\eta f'(x_{old}) \\\\\n",
    "x_{new} = x_{old} - \\frac{f'(x_{old})}{f''(x_{old})}\n",
    "$$\n",
    "\n",
    "可以将牛顿法看作时梯度下降法的改进版，通过二阶导数信息来调整学习率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 variable(2.0)\n",
      "1 variable(1.4545454545454546)\n",
      "2 variable(1.1510467893775467)\n",
      "3 variable(1.0253259289766978)\n",
      "4 variable(1.0009084519430513)\n",
      "5 variable(1.0000012353089454)\n",
      "6 variable(1.000000000002289)\n",
      "7 variable(1.0)\n",
      "8 variable(1.0)\n",
      "9 variable(1.0)\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    y = x ** 4 - 2 * x ** 2\n",
    "    return y\n",
    "\n",
    "def gx2(x):  # 手动计算二阶导数\n",
    "    return 12 * x ** 2 - 4\n",
    "\n",
    "x = Variable(np.array(2.0))\n",
    "iters = 10\n",
    "\n",
    "for i in range(iters):\n",
    "    print(i, x)\n",
    "    y = f(x)\n",
    "    x.cleargrad()\n",
    "    y.backward()\n",
    "    x.data -= x.grad / gx2(x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 30: Achieving Higher Derivatives (Preparation)\n",
    "\n",
    "复习Variable类和Function类的实现，为实现高阶导数做准备。\n",
    "- 计算的连接是在 Function 类的__call__方法中创建的\n",
    "- 正向传播和反向传播的具体计算是在继承了 Function 的类的forward法和 backward方法中进行的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 31: Achieving Higher Derivatives (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "连接是在进行正向传播的计算时创建的，在反向传播时不会被创建。因为该计算针对的是ndarray实例。\n",
    "\n",
    "求导计算换句话说就是通过反向传播进行计算。如果能对反向传播所进行的计算创建连接，就能自动计算高阶导数。\n",
    "\n",
    "为此，我们需要**将导数(梯度)保存为 Variable 实例**！\n",
    "\n",
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"./res/反向传播创建连接.png\" width=\"300\"/></td>\n",
    "    <td><img src=\"./res/反向传播创建连接2.png\" width=\"300\"/></td>\n",
    "  </tr>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 32: Achieving Higher Derivatives (Implementation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
